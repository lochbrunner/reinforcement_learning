{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finite Markov Decision Processes\n",
    "\n",
    "Finite MDP.\n",
    "Difference to previous chapter: State $q_*(a) \\rightarrow q_*(a,s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "Probability of state $s'$ and reward $r$ prior state $s$ and action $a$:\n",
    "\n",
    "\\begin{align}\n",
    "p(s',r|s,a) = \\operatorname{Pr}\\left\\{ S_t=s', R_t=r \\;|\\; S_{t-1}, A_{t-1}=a\\right\\}\n",
    "\\end{align}\n",
    "\n",
    "The *state-transition probabilities*\n",
    "\n",
    "\\begin{align}\n",
    "p(s'|s,a) = \\sum_r p(s',r|s,a)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Recycling Robot\n",
    "\n",
    "**States:** Battery load: *high* and *low*\n",
    "**Actions:** *search*, *wait* and *recharge*\n",
    "\n",
    "#### Reward and probability table\n",
    "\n",
    " $s$ | $a$ | $s'$ | $p(s';s,a)$ | $r(s,a,s')$\n",
    " --- | --- | --- | --- | ---\n",
    " high| search | high | $\\alpha$ | $r_\\text{search}$\n",
    " high| search | low  | $1-\\alpha$ | $r_\\text{search}$\n",
    " low | search | high | $1-\\beta$ | $-3$\n",
    " low | search | low  | $\\beta$  | $r_\\text{search}$\n",
    " high| wait   | high | $1$      | $r_\\text{wait}$\n",
    " high| wait   | low  | $0$      | -\n",
    " low | wait   | high | $0$      | -\n",
    " low | wait   | low  | $1$      | $r_\\text{wait}$\n",
    " low | recharge | high | $1$    | $0$\n",
    " low | recharge | low  | $0$    | -\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dm-env in /home/matthias/.local/lib/python3.8/site-packages (from -r ../requirements.txt (line 2)) (1.5)\n",
      "Requirement already satisfied: numpy in /home/matthias/.local/lib/python3.8/site-packages (from -r ../requirements.txt (line 3)) (1.22.0)\n",
      "Requirement already satisfied: seaborn in /home/matthias/.local/lib/python3.8/site-packages (from -r ../requirements.txt (line 4)) (0.11.2)\n",
      "Requirement already satisfied: absl-py in /home/matthias/.local/lib/python3.8/site-packages (from dm-env->-r ../requirements.txt (line 2)) (0.11.0)\n",
      "Requirement already satisfied: dm-tree in /home/matthias/.local/lib/python3.8/site-packages (from dm-env->-r ../requirements.txt (line 2)) (0.1.6)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /usr/lib/python3/dist-packages (from seaborn->-r ../requirements.txt (line 4)) (3.1.2)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/matthias/.local/lib/python3.8/site-packages (from seaborn->-r ../requirements.txt (line 4)) (1.5.2)\n",
      "Requirement already satisfied: pandas>=0.23 in /home/matthias/.local/lib/python3.8/site-packages (from seaborn->-r ../requirements.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from absl-py->dm-env->-r ../requirements.txt (line 2)) (1.14.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/lib/python3/dist-packages (from pandas>=0.23->seaborn->-r ../requirements.txt (line 4)) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/lib/python3/dist-packages (from pandas>=0.23->seaborn->-r ../requirements.txt (line 4)) (2.7.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dm_env\n",
    "from dm_env import specs\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Sequence, Any\n",
    "\n",
    "\n",
    "@dataclass(frozen=True, eq=True)\n",
    "class StateAction:\n",
    "    state: str\n",
    "    action: str\n",
    "\n",
    "\n",
    "@dataclass(frozen=True, eq=True)\n",
    "class NextStates:\n",
    "    state_a: str\n",
    "    state_b: str\n",
    "    reward_a: float\n",
    "    reward_b: float\n",
    "    probability_a: float\n",
    "\n",
    "    def next(self, rng: np.random.RandomState) -> Tuple[str, float]:\n",
    "        if rng.random() <= self.probability_a:\n",
    "            return (self.state_a, self.reward_a)\n",
    "        else:\n",
    "            return (self.state_b, self.reward_b)\n",
    "\n",
    "\n",
    "class RecyclingRobot(dm_env.Environment):\n",
    "    def __init__(self, alpha: float = 0.9, beta: float = 0.95, r_search: float = 1.0, r_wait: float = 0., seed: int = 1) -> None:\n",
    "        self._seed = seed\n",
    "        self._reset()\n",
    "\n",
    "        self._actions = ['search', 'wait', 'recharge']\n",
    "        self._transitions = {\n",
    "            StateAction('high', 'search'): NextStates('high', 'low', r_search, r_search, alpha),\n",
    "            StateAction('low', 'search'): NextStates('high', 'low', -3, r_search, 1. - beta),\n",
    "            StateAction('high', 'wait'): NextStates('high', 'low', r_wait, 0., 1.),\n",
    "            StateAction('low', 'wait'): NextStates('high', 'low', 0, r_wait, 0.),\n",
    "            StateAction('low', 'recharge'): NextStates('high', 'low', 0., 0., 1.),\n",
    "        }\n",
    "\n",
    "    def reset(self) -> dm_env.TimeStep:\n",
    "        self._reset()\n",
    "        return dm_env.restart(reward=0., observation=np.asarray([0]))\n",
    "\n",
    "    def step(self, action) -> dm_env.TimeStep:\n",
    "        action_name = self._actions[action]\n",
    "        prior = StateAction(self.state, action_name)\n",
    "        self.state, reward = self._transitions[prior].next(self._rng)\n",
    "        observation = self._actions.index(self.state)\n",
    "\n",
    "        # when to terminate?\n",
    "        # return dm_env.termination(reward=reward, observation=observation)\n",
    "\n",
    "        return dm_env.transition(reward=reward, observation=observation, discount=0.9)\n",
    "\n",
    "    def observation_spec(self) -> specs.BoundedArray:\n",
    "        return specs.BoundedArray(\n",
    "            shape=(1,),\n",
    "            dtype=np.float_,\n",
    "            name=\"recycling-robot\",\n",
    "            minimum=0,\n",
    "            maximum=2,\n",
    "        )\n",
    "\n",
    "    def action_spec(self) -> specs.DiscreteArray:\n",
    "        return specs.DiscreteArray(dtype=int, num_values=1, name=\"action\")\n",
    "\n",
    "    def _reset(self):\n",
    "        self._rng = np.random.RandomState(self._seed)\n",
    "        self.state = 'high'\n",
    "\n",
    "    @property\n",
    "    def available_states(self) -> Sequence[Any]:\n",
    "        \"\"\"This function is needed when iterating over all states\"\"\"\n",
    "        \n",
    "        yield 'low'\n",
    "        yield 'high'\n",
    "\n",
    "    def state_index(self, state: np.array) -> int:\n",
    "        return state[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returns and Episodes\n",
    "\n",
    "Return with discount $\\gamma$ is given by:\n",
    "\n",
    "\\begin{align}\n",
    "G_t := \\sum_{k=0}^\\infty \\gamma^k R_{t+t+1} = R_{t+1} + \\gamma G_{t+1}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy and Value Functions\n",
    "\n",
    "The *value-function* under policy $\\pi$\n",
    "\n",
    "\\begin{align}\n",
    "v_\\pi(s) :=&\\; \\mathbb{E}\\left[G_t\\;|\\; S_t=s\\right] \\\\\n",
    "=&\\; \\sum_a \\pi(a|s)\\sum_{s',r} p(s',r\\,|\\,s,a)\\left[r+\\gamma v_\\pi(s') \\right]\n",
    "\\end{align}\n",
    "\n",
    "The *action-value-function* \n",
    "\\begin{align}\n",
    "q_\\pi(s,a) :=&\\; \\mathbb{E}\\left[G_t\\;|\\; S_t=s S_t=a\\right]\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Any, Dict\n",
    "\n",
    "\n",
    "@dataclass(frozen=True, eq=True)\n",
    "class Position:\n",
    "    x: int\n",
    "    y: int\n",
    "\n",
    "    def numpy(self):\n",
    "        return np.asarray([self.y, self.x])\n",
    "\n",
    "    @staticmethod\n",
    "    def from_numpy(array: np.array) -> \"Position\":\n",
    "        assert array.shape == (2,)\n",
    "        return Position(array[1], array[0])\n",
    "\n",
    "    def __iadd__(self, other: \"Position\") -> \"Position\":\n",
    "        self.x += other.x\n",
    "        self.y += other.y\n",
    "        return self\n",
    "\n",
    "    def __add__(self, other: \"Position\") -> \"Position\":\n",
    "        return Position(self.x + other.x, self.y + other.y)\n",
    "\n",
    "    def is_in_rectangle(self, height: int, width: int) -> bool:\n",
    "        return width > self.x >= 0 and height > self.y >= 0\n",
    "\n",
    "\n",
    "@dataclass(frozen=True, eq=True)\n",
    "class BeamAction(Position):\n",
    "    reward: float\n",
    "\n",
    "\n",
    "class GridWorld(dm_env.Environment):\n",
    "\n",
    "    def __init__(self, width: int, height: int, beam_actions: Dict[Position, BeamAction],\n",
    "                 initial: Position = None, discount: float = 0.9, seed: int = 1) -> None:\n",
    "        self._seed = seed\n",
    "        self._discount = discount\n",
    "        self._width = width\n",
    "        self._height = height\n",
    "        self._beam_actions = beam_actions\n",
    "        self._initial = initial or Position(0, 0)\n",
    "        self._reset()\n",
    "\n",
    "        self._actions = {\n",
    "            'north': Position(0, -1),\n",
    "            'south': Position(0, 1),\n",
    "            'west': Position(-1, 0),\n",
    "            'east': Position(1, 0)\n",
    "        }\n",
    "\n",
    "        self.action_names = sorted(list(self._actions.keys()))\n",
    "\n",
    "    def step(self, action) -> dm_env.TimeStep:\n",
    "        # reward of -1 if walking out of boundaries\n",
    "        # reward of +10 when reaching a_source\n",
    "        # reward of +5 when reaching b_source\n",
    "        # no reward else\n",
    "        delta = self._actions[self.action_names[action]]\n",
    "        candidate = self.state + delta\n",
    "        if candidate in self._beam_actions:\n",
    "            beam = self._beam_actions[candidate]\n",
    "            reward = beam.reward\n",
    "            self.state = Position(beam.x, beam.y)\n",
    "        elif candidate.is_in_rectangle(self._height, self._width):\n",
    "            reward = 0\n",
    "            self.state = candidate\n",
    "        else:\n",
    "            reward = -1\n",
    "        return dm_env.transition(reward=reward, observation=self.state.numpy(), discount=self._discount)\n",
    "\n",
    "    def reset(self) -> dm_env.TimeStep:\n",
    "        self._reset()\n",
    "        return dm_env.restart(reward=0., observation=self.state.numpy())\n",
    "\n",
    "    def observation_spec(self) -> specs.BoundedArray:\n",
    "        return specs.BoundedArray(\n",
    "            shape=(2,),  # current locations\n",
    "            dtype=np.int32,\n",
    "            name=\"grid-world\",\n",
    "            minimum=max(self._width, self._height),\n",
    "            maximum=0,\n",
    "        )\n",
    "\n",
    "    def action_spec(self) -> specs.DiscreteArray:\n",
    "        return specs.DiscreteArray(dtype=int, num_values=1, name=\"action\")\n",
    "\n",
    "    def _reset(self):\n",
    "        self._rng = np.random.RandomState(self._seed)\n",
    "        self.state = self._initial\n",
    "\n",
    "    @property\n",
    "    def available_states(self) -> Sequence[Any]:\n",
    "        \"\"\"This function is needed when iterating over all states\"\"\"\n",
    "        for x in range(self._width):\n",
    "            for y in range(self._height):\n",
    "                candidate = Position(x, y)\n",
    "                # if candidate == self._a_source or candidate == self._b_source:\n",
    "                #     continue\n",
    "                yield candidate\n",
    "\n",
    "    def state_index(self, state: np.array) -> int:\n",
    "        location = Position.from_numpy(state)\n",
    "        return location.x * self._width + location.y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Policies and Optimal Value Functions\n",
    "\n",
    "The optimal *state-value-function* is defined \n",
    "\n",
    "\\begin{align}\n",
    "v_*(s) := \\max_\\pi v_\\pi(s)\n",
    "\\end{align}\n",
    "\n",
    "The optimal *action-value-function* is defined\n",
    "\n",
    "\\begin{align}\n",
    "q_*(s, a) :=& \\max_\\pi q_\\pi(s, a) \\\\\n",
    "=& \\sum_{s',r} p(s',r\\,|\\, s, a) \\left[r+\\gamma \\max_{a'}q_*(s', a') \\right]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the Grid-World\n",
    "\n",
    "Finding the optimal *state-value-function* leads to the optimal optimal *policy-function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 116995.93it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.35467178  0.         -0.90876411  0.         -2.6161441 ]\n",
      " [-2.09589488  0.          0.          0.         -0.85427771]\n",
      " [-5.11390602  0.          0.         14.00744842 -2.34259947]\n",
      " [-2.80039111  0.          0.          0.         -2.61680176]\n",
      " [-7.27663664 13.28515612 -1.35618216 -1.06345616 -6.16948784]]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "def softmax(values: np.array):\n",
    "    exp = np.exp(values)\n",
    "    return exp / np.sum(exp)\n",
    "\n",
    "\n",
    "WIDTH = 5\n",
    "HEIGHT = 5\n",
    "\n",
    "beam_actions = {\n",
    "    Position(1, 0): BeamAction(1, 4, reward=10.),\n",
    "    Position(3, 0): BeamAction(3, 2, reward=5.),\n",
    "}\n",
    "\n",
    "env = GridWorld(width=WIDTH, height=HEIGHT, beam_actions=beam_actions)\n",
    "\n",
    "STEPS = 1000\n",
    "GAMMA = 0.9\n",
    "\n",
    "rng = np.random.RandomState(seed=0)\n",
    "values = np.zeros((HEIGHT, WIDTH))\n",
    "\n",
    "for _ in tqdm(range(STEPS)):\n",
    "    action = rng.randint(len(env.action_names))\n",
    "    step = env.step(action)\n",
    "    ob = tuple(step.observation)\n",
    "    values[ob] *= GAMMA\n",
    "    values[ob] += step.reward\n",
    "\n",
    "\n",
    "ax = sns.heatmap(values, linewidth=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
